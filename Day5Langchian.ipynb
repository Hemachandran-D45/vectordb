{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqvols5n5Gmhf8JYct56vH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hemachandran-D45/vectordb/blob/main/Day5Langchian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pldllOkz21f7"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is new library announced by hugging face"
      ],
      "metadata": {
        "id": "oiFSd1DS3ThZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!pip install langchain\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "TlBdVQp6254t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging face hub has repositories of various models which you can specifically call from hugging face\n",
        "Transformer important cuz if i probably go ahead and search respect any model in hugging face"
      ],
      "metadata": {
        "id": "6ag55KRd3o5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistral_inference\n"
      ],
      "metadata": {
        "id": "ytA9i31bFLDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_core langchain_huggingface"
      ],
      "metadata": {
        "id": "Ocrj4A_dJfDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Environment Secret keys\n",
        "\n",
        "from google.colab import userdata\n",
        "sec_key = userdata.get(\"Langchain_Testing\")"
      ],
      "metadata": {
        "id": "kh0XlI3r7sZx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling a model in two ways\n",
        "1) Directly downloading a model in local\n",
        "2) Can call this model through endpoint"
      ],
      "metadata": {
        "id": "LL6dGOes_1js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face Endpoint\n",
        "\n",
        "Access Hugging Face Model with API (Hugging Face Hub)\n",
        "There are two ways to use this class. You can specify the model with repo_id parameter. Those endpoints use the serverless API which is pariculary beneficial to people using pro account. Still, regular user can already have access to fair amount of request with their Hugging Face Token in the environment where they are executing the code.\n"
      ],
      "metadata": {
        "id": "6tWm4DlLAHSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "kYDWb1iY-9jh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "sec_key = userdata.get(\"Langchain_Testing\")"
      ],
      "metadata": {
        "id": "kNWdeJMhJAS-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = sec_key"
      ],
      "metadata": {
        "id": "1x1cvy14-_ib"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "based on the authentication we will be able to call any model from hugging face"
      ],
      "metadata": {
        "id": "xMDxxnMfCEwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    max_length=128,\n",
        "    temperature=0.7,\n",
        "    token=sec_key\n",
        ")"
      ],
      "metadata": {
        "id": "l4pR4or5Cpal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"I want to start a restaurant for Mexican Food, Suggest a fency name \")"
      ],
      "metadata": {
        "id": "aAa_RUxcJEhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  response = llm.invoke(\"What is machine learnig\")\n",
        "  print(response)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "ELhZzpiyDTil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import template\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "question = \"I want to oepn an restaurant for Indian Food, suggest a fancy name.\"\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "zP3xJVeHDUGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Langchain we have prompt template which actually help us to create your own custom and we can entirely use kind of RAG application. Retrieval Argument Generation, where we can provide our question"
      ],
      "metadata": {
        "id": "NM0yCdBXYwRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain=LLMChain(prompt=prompt, llm=llm)\n",
        "response = llm_chain.run(question)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "tJDbDEWzSfMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llm is nothing but that model mistral , prompt is our prompt."
      ],
      "metadata": {
        "id": "__Ay-mgxXqqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face Pipeline\n",
        "\n",
        "Among transformer,the pipeline is the most versatile tool in the Hugging Face  toolbox. Langchain being designed primarily to address RAG an Agent use cases, the scope of the pipeline here is reduced to the following text-centric task. \"text-generation\",\"text2text-generation\",\"summarization\",\"translation\". Models can be loaded directly with the from_model_id method"
      ],
      "metadata": {
        "id": "KDrsSvg_arh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "if we want to download entire model in local environment that time we can use Hugging Face Pipeline but the one disadvantage is Very Big LLM model or multi model cannot be downloaded locally due to RAM issue"
      ],
      "metadata": {
        "id": "gChFYSYVbhqR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXiEsTsNdt-E"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM,AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "6G63AQEEatX5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automodel for causal Lm - we can import any model just by using the function prtrained , for every model there will be tokenizer (take a input and convert that into some embedding vectors)"
      ],
      "metadata": {
        "id": "VJ13mA9Xc4FD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id=\"gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "0wqaXfUOez52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kind of task is the first parameter\n",
        "\n",
        "pipe = pipeline(\"text-generation\",model=model,tokenizer=tokenizer,max_new_tokens=100)\n",
        "HFP = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "jK-okOv9gzVq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HFP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1Uou0PCh9D5",
        "outputId": "c39b014a-b2b3-4e15-f61b-e12c8705225d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7a4495a5e560>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HFP.invoke(\"Langchain is a company\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "ybloRc02h-SF",
        "outputId": "48c655a1-caba-4179-c761-fcde1a736243"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Langchain is a company that is also involved in other big banks with the \"Fannie Mae and Freddie Mac\" bailout program.\\n\\nAs part of the deal, Goldman\\'s chief of staff, Marc Benioff, was paid $1.1 billion, with an additional $1.4 billion for \"sustainable compliance\" services. Goldman paid Goldman Sachs $1m for \"investement and enforcement consulting\" and $1m for \"non-financial financial advisory and advisory services\" in 2013. That same year'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ryP_5GSiM1s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}