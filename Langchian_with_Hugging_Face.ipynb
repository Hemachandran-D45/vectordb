{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9yiSlE5D8JZwcKavc9CSH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hemachandran-D45/vectordb/blob/main/Langchian_with_Hugging_Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pldllOkz21f7"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-huggingface -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is new library announced by hugging face"
      ],
      "metadata": {
        "id": "oiFSd1DS3ThZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!pip install langchain\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "TlBdVQp6254t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging face hub has repositories of various models which you can specifically call from hugging face\n",
        "Transformer important cuz if i probably go ahead and search respect any model in hugging face"
      ],
      "metadata": {
        "id": "6ag55KRd3o5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistral_inference -q\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ytA9i31bFLDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_core langchain_huggingface"
      ],
      "metadata": {
        "id": "Ocrj4A_dJfDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling a model in two ways\n",
        "1) Directly downloading a model in local\n",
        "2) Can call this model through endpoint"
      ],
      "metadata": {
        "id": "LL6dGOes_1js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face Endpoint\n",
        "\n",
        "Access Hugging Face Model with API (Hugging Face Hub)\n",
        "There are two ways to use this class. You can specify the model with repo_id parameter. Those endpoints use the serverless API which is pariculary beneficial to people using pro account. Still, regular user can already have access to fair amount of request with their Hugging Face Token in the environment where they are executing the code.\n"
      ],
      "metadata": {
        "id": "6tWm4DlLAHSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "kYDWb1iY-9jh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Environment for Secret Key\n",
        "\n",
        "from google.colab import userdata\n",
        "sec_key = userdata.get(\"Langchain_Testing\")"
      ],
      "metadata": {
        "id": "kNWdeJMhJAS-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = sec_key"
      ],
      "metadata": {
        "id": "1x1cvy14-_ib"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "based on the authentication we will be able to call any model from hugging face"
      ],
      "metadata": {
        "id": "xMDxxnMfCEwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    max_length=128,\n",
        "    temperature=0.7,\n",
        "    token=sec_key\n",
        ")"
      ],
      "metadata": {
        "id": "l4pR4or5Cpal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"I want to start a restaurant for Indian food, Suggest a fancy name \")"
      ],
      "metadata": {
        "id": "aAa_RUxcJEhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  response = llm.invoke(\"What is machine learnig\")\n",
        "  print(response)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "ELhZzpiyDTil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import template\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "question = \"I want to oepn an restaurant for Indian Food, suggest a fancy name.\"\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "zP3xJVeHDUGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Langchain we have prompt template which actually help us to create your own custom and we can entirely use kind of RAG application. Retrieval Argument Generation, where we can provide our question"
      ],
      "metadata": {
        "id": "NM0yCdBXYwRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain=LLMChain(prompt=prompt, llm=llm)\n",
        "response = llm_chain.run(question)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "tJDbDEWzSfMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llm is nothing but that model mistral , prompt is our prompt."
      ],
      "metadata": {
        "id": "__Ay-mgxXqqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face Pipeline\n",
        "\n",
        "Among transformer,the pipeline is the most versatile tool in the Hugging Face  toolbox. Langchain being designed primarily to address RAG an Agent use cases, the scope of the pipeline here is reduced to the following text-centric task. \"text-generation\",\"text2text-generation\",\"summarization\",\"translation\". Models can be loaded directly with the from_model_id method"
      ],
      "metadata": {
        "id": "KDrsSvg_arh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "if we want to download entire model in local environment that time we can use Hugging Face Pipeline but the one disadvantage is Very Big LLM model or multi model cannot be downloaded locally due to RAM issue"
      ],
      "metadata": {
        "id": "gChFYSYVbhqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM,AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "6G63AQEEatX5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automodel for causal Lm - we can import any model just by using the function pretrained , for every model there will be tokenizer (take a input and convert that into some embedding vectors)"
      ],
      "metadata": {
        "id": "VJ13mA9Xc4FD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id=\"gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "0wqaXfUOez52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kind of task is the first parameter\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                max_new_tokens=100)\n",
        "HFP = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "jK-okOv9gzVq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HFP  ## model"
      ],
      "metadata": {
        "id": "Y1Uou0PCh9D5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9135438-c551-48a7-e6f0-a389044ccce0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7d3dc94c8a30>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HFP.invoke(\"What is Machine Learning\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "ybloRc02h-SF",
        "outputId": "ca747f25-017b-4c9d-f99b-af6cf3fb7ab5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"What is Machine Learning?\\n\\nMachine learning is a tool for learning, which means that it doesn't require any programming skill. It lets you develop predictive models that can lead to better outcomes. While using Machine Learning is certainly a viable alternative to Artificial Intelligence as a general purpose software platform, it can be very time consuming and takes a lot of time as well. The primary goal of ML projects has been to help provide a robust, scalable, and extensible programming language for all applications in robotics, aerospace, and medical\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmLMY6iPwl5E",
        "outputId": "47306f88-f881-4fc4-ead7-4a4e2003ffb0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Use the Hugging Face Pipeline with CPU\n",
        "\n",
        "gpu_llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"gpt2\",\n",
        "    task=\"text-generation\",\n",
        "    device=-1,\n",
        "    pipeline_kwargs = {\"max_new_tokens\":100}\n",
        "  )"
      ],
      "metadata": {
        "id": "_ryP_5GSiM1s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73007691-e8cc-4195-b2da-3715a87223da"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"Question : {question}\"\n",
        "prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "A3dBcjeoyNNn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain=prompt|gpu_llm #another way of creating LLM Chain"
      ],
      "metadata": {
        "id": "1OShToz8y5Mx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is Artificial Intelligence\"\n",
        "chain.invoke({\"question\" : question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "dyOs4uiiy_wK",
        "outputId": "3928ee7f-1b45-4ed2-fffa-42558c0e56ba"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Question : What is Artificial Intelligence?\\nThe core of Artificial Intelligence is called \"Artificial Intelligence\" or AI. This term refers to some of the techniques and concepts that we used to learn about Artificial Intelligence (AI).\\nArtificial Intelligence uses the techniques and concepts associated with robotics (Birds and dinosaurs), visual, acoustic and motion sensors, actuators, and other types of tools. Most recent innovations include quantum computer, distributed systems, deep learning, computer graphics, and artificial intelligence, which are all ways of helping create more'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2nrZg8qr2fIh"
      }
    }
  ]
}