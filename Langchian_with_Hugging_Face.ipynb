{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN93hH098Ljx8Vx5BHkaKJl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hemachandran-D45/vectordb/blob/main/Langchian_with_Hugging_Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pldllOkz21f7"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is new library announced by hugging face"
      ],
      "metadata": {
        "id": "oiFSd1DS3ThZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!pip install langchain\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "TlBdVQp6254t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging face hub has repositories of various models which you can specifically call from hugging face\n",
        "Transformer important cuz if i probably go ahead and search respect any model in hugging face"
      ],
      "metadata": {
        "id": "6ag55KRd3o5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistral_inference\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ytA9i31bFLDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_core langchain_huggingface"
      ],
      "metadata": {
        "id": "Ocrj4A_dJfDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling a model in two ways\n",
        "1) Directly downloading a model in local\n",
        "2) Can call this model through endpoint"
      ],
      "metadata": {
        "id": "LL6dGOes_1js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face Endpoint\n",
        "\n",
        "Access Hugging Face Model with API (Hugging Face Hub)\n",
        "There are two ways to use this class. You can specify the model with repo_id parameter. Those endpoints use the serverless API which is pariculary beneficial to people using pro account. Still, regular user can already have access to fair amount of request with their Hugging Face Token in the environment where they are executing the code.\n"
      ],
      "metadata": {
        "id": "6tWm4DlLAHSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "kYDWb1iY-9jh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Environment for Secret Key\n",
        "\n",
        "from google.colab import userdata\n",
        "sec_key = userdata.get(\"Langchain_Testing\")"
      ],
      "metadata": {
        "id": "kNWdeJMhJAS-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = sec_key"
      ],
      "metadata": {
        "id": "1x1cvy14-_ib"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "based on the authentication we will be able to call any model from hugging face"
      ],
      "metadata": {
        "id": "xMDxxnMfCEwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    max_length=128,\n",
        "    temperature=0.7,\n",
        "    token=sec_key\n",
        ")"
      ],
      "metadata": {
        "id": "l4pR4or5Cpal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"I want to start a restaurant for Indian food, Suggest a fancy name \")"
      ],
      "metadata": {
        "id": "aAa_RUxcJEhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  response = llm.invoke(\"What is machine learnig\")\n",
        "  print(response)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "ELhZzpiyDTil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import template\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "question = \"I want to oepn an restaurant for Indian Food, suggest a fancy name.\"\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zP3xJVeHDUGG",
        "outputId": "5ca444a1-4f4b-4d23-8855-374f7818f5b9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['question'] input_types={} partial_variables={} template=\"Question: {question}\\n\\nAnswer: Let's think step by step.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Langchain we have prompt template which actually help us to create your own custom and we can entirely use kind of RAG application. Retrieval Argument Generation, where we can provide our question"
      ],
      "metadata": {
        "id": "NM0yCdBXYwRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain=LLMChain(prompt=prompt, llm=llm)\n",
        "response = llm_chain.run(question)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJDbDEWzSfMK",
        "outputId": "9981f9a2-8610-45a8-b643-c18025418a2e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A fancy name for an Indian restaurant should be unique, catchy, and evoke a sense of sophistication and elegance. Here are some suggestions:\n",
            "\n",
            "1. \"Saffron Splendour\" - Saffron is a key ingredient in Indian cuisine and symbolizes royalty, luxury, and generosity.\n",
            "2. \"Mystic Maharajah\" - Maharajah refers to an Indian ruler, suggesting a royal experience. \"Mystic\" adds a touch of mystery and intrigue.\n",
            "3. \"Royal Rajah's Table\" - This name emphasizes the royal theme and \"table\" suggests a dining experience.\n",
            "4. \"Spice Symphony\" - This name plays on the harmonious blend of spices in Indian cuisine and the musical term \"symphony.\"\n",
            "5. \"Diamond Darbar\" - Darbar means \"court\" or \"palace\" in Hindi. Combining it with \"Diamond\" creates an elegant and luxurious image.\n",
            "6. \"Gourmet Ghar\" - Ghar means \"home\" in Hindi, but Gourmet Ghar suggests a high-end, elegant dining experience.\n",
            "7. \"Bollywood Banquet\" - Bollywood is famous for its glamour and extravagance, and \"banquet\" suggests a grand feast.\n",
            "8. \"Taj Temptations\" - Taj is a famous Indian luxury hotel brand, evoking images of opulence and elegance.\n",
            "9. \"Rasoi Royal\" - Rasoi means \"kitchen\" in Hindi, but \"Royal\" suggests a regal dining experience.\n",
            "10. \"Curry Castle\" - This name plays on the popular dish \"curry\" and the idea of a castle, suggesting a grand and luxurious dining experience.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "llm is nothing but that model mistral , prompt is our prompt."
      ],
      "metadata": {
        "id": "__Ay-mgxXqqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face Pipeline\n",
        "\n",
        "Among transformer,the pipeline is the most versatile tool in the Hugging Face  toolbox. Langchain being designed primarily to address RAG an Agent use cases, the scope of the pipeline here is reduced to the following text-centric task. \"text-generation\",\"text2text-generation\",\"summarization\",\"translation\". Models can be loaded directly with the from_model_id method"
      ],
      "metadata": {
        "id": "KDrsSvg_arh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "if we want to download entire model in local environment that time we can use Hugging Face Pipeline but the one disadvantage is Very Big LLM model or multi model cannot be downloaded locally due to RAM issue"
      ],
      "metadata": {
        "id": "gChFYSYVbhqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM,AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "6G63AQEEatX5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automodel for causal Lm - we can import any model just by using the function pretrained , for every model there will be tokenizer (take a input and convert that into some embedding vectors)"
      ],
      "metadata": {
        "id": "VJ13mA9Xc4FD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id=\"gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "0wqaXfUOez52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kind of task is the first parameter\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                max_new_tokens=100)\n",
        "HFP = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "jK-okOv9gzVq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HFP"
      ],
      "metadata": {
        "id": "Y1Uou0PCh9D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HFP.invoke(\"What is Machine Learning\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "ybloRc02h-SF",
        "outputId": "f82aac81-29f4-4fe1-cc1d-fa0497af5777"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is Machine Learning?\\n\\nMachine learning is a way to train objects to create, transform, and perform computations. Machine learning is useful when you want to improve the performance of specific algorithms, but it can add some weight depending on your needs to your project.\\n\\nThe benefits of machine learning are simple:\\n\\nBetter performance: Machine learning is an algorithm which learns to perform a task more quickly than the human would expect.\\n\\nBetter learning speed: The slower you learn, the faster you can accomplish'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmLMY6iPwl5E",
        "outputId": "f335d69a-b76f-4774-a7c3-8596a6e1287d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Use the Hugging Face Pipeline with CPU\n",
        "\n",
        "gpu_llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"gpt2\",\n",
        "    task=\"text-generation\",\n",
        "    device=-1,\n",
        "    pipeline_kwargs = {\"max_new_tokens\":100}\n",
        "  )"
      ],
      "metadata": {
        "id": "_ryP_5GSiM1s"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"Question : {question}\"\n",
        "prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "A3dBcjeoyNNn"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain=prompt|gpu_llm"
      ],
      "metadata": {
        "id": "1OShToz8y5Mx"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is Machine Learning\"\n",
        "chain.invoke({\"question\" : question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "dyOs4uiiy_wK",
        "outputId": "d1a6d4df-fdcb-4623-ae9e-60f9a76d25ef"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Question: What is Machine Learning and what are the benefits?\\n\\nThe Machine Learning benefits are quite profound: you can use your model (such as neural networks or artificial intelligence) to understand data more easily, improve your learning curve, and be more efficient in solving problems much faster, all in the face of powerful, predictive AI that can even detect large numbers of clues.\\n\\nSo, Machine learning, as a tool, is pretty important, as a tool, it's the only tool that supports its own set of insights\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2nrZg8qr2fIh"
      }
    }
  ]
}